"""
Feedbackå®Ÿé¨“ç”¨SAEåˆ†æå™¨

ã“ã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã¯ã€feedback.jsonlãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½¿ç”¨ã—ã¦ã€LLMã®ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã«å¯¾ã™ã‚‹
å¿œç­”ã¨ãã®éš›ã®SAEå†…éƒ¨çŠ¶æ…‹ã‚’åˆ†æã—ã¾ã™ã€‚
"""

import json
import os
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Any
from dataclasses import dataclass, asdict
import torch
import numpy as np
from datetime import datetime
from tqdm import tqdm

# SAE Lens imports
from transformer_lens import HookedTransformer
from sae_lens import SAE


@dataclass
class FeedbackPromptInfo:
    """ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæƒ…å ±"""
    dataset: str
    prompt_template_type: str
    prompt: str
    base_data: Dict[str, Any]  # å…ƒã®baseãƒ‡ãƒ¼ã‚¿ã‚’ä¿æŒ


@dataclass
class FeedbackResponse:
    """1ã¤ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«å¯¾ã™ã‚‹å¿œç­”ã¨SAEçŠ¶æ…‹"""
    prompt_info: FeedbackPromptInfo
    response_text: str
    sae_activations: Dict[str, Any]  # {feature_id: activation_value}
    top_k_features: List[Tuple[int, float]]  # [(feature_id, value), ...]
    metadata: Dict[str, Any]


@dataclass
class FeedbackQuestionResult:
    """1ã¤ã®è³ªå•ï¼ˆ5ã¤ã®ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ï¼‰ã®åˆ†æçµæœ"""
    question_id: int
    dataset: str
    base_text: str
    variations: List[FeedbackResponse]
    timestamp: str


class FeedbackAnalyzer:
    """Feedbackå®Ÿé¨“ç”¨ã®SAEåˆ†æå™¨"""
    
    def __init__(self, config):
        """
        åˆæœŸåŒ–
        
        Args:
            config: ExperimentConfig ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ
        """
        self.config = config
        self.model = None
        self.sae = None
        self.results: List[FeedbackQuestionResult] = []
        
        # Feedbackå°‚ç”¨è¨­å®šã®å–å¾—
        self.feedback_config = getattr(config, 'feedback', None)
        if self.feedback_config is None:
            # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’è¨­å®š
            from config import FeedbackConfig
            self.feedback_config = FeedbackConfig()
        
        # çµæœä¿å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ä½œæˆ
        self.results_dir = Path("results/feedback")
        self.results_dir.mkdir(parents=True, exist_ok=True)
        
        if self.config.debug.verbose:
            print("ğŸ”§ FeedbackAnalyzer initialized")
            print(f"   ğŸ“ Results directory: {self.results_dir}")
            print(f"   ğŸ’¾ Save all tokens: {self.feedback_config.save_all_tokens}")
            print(f"   ğŸ¯ Target layer: {self.feedback_config.target_layer}")
    
    def load_feedback_data(self, data_path: Optional[str] = None) -> List[Dict]:
        """
        feedback.jsonlãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€
        
        Args:
            data_path: ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ï¼ˆNoneã®å ´åˆã¯configã‹ã‚‰å–å¾—ï¼‰
        
        Returns:
            èª­ã¿è¾¼ã‚“ã ãƒ‡ãƒ¼ã‚¿ã®ãƒªã‚¹ãƒˆ
        """
        if data_path is None:
            data_path = self.config.data.dataset_path
        
        if self.config.debug.verbose:
            print(f"ğŸ“‚ Loading feedback data from: {data_path}")
        
        with open(data_path, 'r', encoding='utf-8') as f:
            data = [json.loads(line) for line in f]
        
        if self.config.debug.verbose:
            print(f"âœ… Loaded {len(data)} entries")
        
        return data
    
    def create_prompt(self, data: Dict) -> FeedbackPromptInfo:
        """
        ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæƒ…å ±ã‚’ä½œæˆ
        
        Args:
            data: feedback.jsonlã®1ã‚¨ãƒ³ãƒˆãƒª
        
        Returns:
            FeedbackPromptInfo ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ
        """
        dataset = data["base"]["dataset"]
        metadata = data["metadata"]
        prompt_template = metadata["prompt_template"]
        prompt_template_type = metadata["prompt_template_type"]
        
        if dataset == "arguments" or dataset == "poems":
            text = data["base"]["text"]
            prompt = prompt_template.format(text=text)
        elif dataset == "math":
            question = data["base"]["question"]
            correct_solution = data["base"]["correct_solution"]
            prompt = prompt_template.format(
                question=question, 
                correct_solution=correct_solution
            )
        else:
            raise ValueError(f"Unknown dataset: {dataset}")
        
        return FeedbackPromptInfo(
            dataset=dataset,
            prompt_template_type=prompt_template_type,
            prompt=prompt,
            base_data=data["base"]
        )
    
    def aggregate_prompts(self, feedback_data: List[Dict]) -> List[List[FeedbackPromptInfo]]:
        """
        ãƒ‡ãƒ¼ã‚¿ã‚’5ã¤ã®ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ã”ã¨ã«ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
        
        Args:
            feedback_data: feedback.jsonlã®å…¨ãƒ‡ãƒ¼ã‚¿
        
        Returns:
            [[variation1, variation2, ..., variation5], ...] ã®å½¢å¼
        """
        prompt_variations = []
        prompt_groups = []
        
        for i, data in enumerate(feedback_data, 1):
            prompt_info = self.create_prompt(data)
            prompt_variations.append(prompt_info)
            
            # 5ã¤ã”ã¨ã«ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
            if i % 5 == 0:
                prompt_groups.append(prompt_variations)
                prompt_variations = []
        
        # æ®‹ã‚ŠãŒã‚ã‚‹å ´åˆï¼ˆãƒ‡ãƒ¼ã‚¿ãŒ5ã®å€æ•°ã§ãªã„å ´åˆï¼‰
        if prompt_variations:
            prompt_groups.append(prompt_variations)
        
        if self.config.debug.verbose:
            print(f"ğŸ“¦ Grouped into {len(prompt_groups)} question sets")
        
        return prompt_groups
    
    def load_model_and_sae(self):
        """ãƒ¢ãƒ‡ãƒ«ã¨SAEã‚’ãƒ­ãƒ¼ãƒ‰"""
        if self.config.debug.verbose:
            print("ğŸ”„ Loading model and SAE...")
        
        # ãƒ‡ãƒã‚¤ã‚¹è¨­å®š
        device = self.config.model.device
        if device == "auto":
            device = "cuda" if torch.cuda.is_available() else "cpu"
        
        if self.config.debug.verbose:
            print(f"   ğŸ–¥ï¸  Using device: {device}")
        
        # ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰
        if self.config.debug.verbose:
            print(f"   ğŸ“¥ Loading model: {self.config.model.name}")
        
        dtype = torch.bfloat16 if getattr(self.config.model, 'use_bfloat16', False) else torch.float16
        
        self.model = HookedTransformer.from_pretrained(
            self.config.model.name,
            device=device,
            dtype=dtype
        )
        
        # SAEã®ãƒ­ãƒ¼ãƒ‰
        if self.config.debug.verbose:
            print(f"   ğŸ“¥ Loading SAE: {self.config.model.sae_release}/{self.config.model.sae_id}")
        
        self.sae, _, _ = SAE.from_pretrained(
            release=self.config.model.sae_release,
            sae_id=self.config.model.sae_id,
            device=device
        )
        
        if self.config.debug.verbose:
            print("âœ… Model and SAE loaded successfully")
            if torch.cuda.is_available():
                memory_allocated = torch.cuda.memory_allocated() / 1e9
                print(f"   ğŸ’¾ GPU Memory: {memory_allocated:.2f} GB")
    
    def generate_with_sae(self, prompt: str) -> Tuple[str, Dict[str, Any]]:
        """
        ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«å¯¾ã—ã¦ç”Ÿæˆã‚’å®Ÿè¡Œã—ã€SAEæ´»æ€§åŒ–ã‚’å–å¾—
        
        Args:
            prompt: å…¥åŠ›ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
        
        Returns:
            (ç”Ÿæˆãƒ†ã‚­ã‚¹ãƒˆ, SAEæ´»æ€§åŒ–æƒ…å ±)
        """
        # ãƒˆãƒ¼ã‚¯ãƒ³åŒ–
        tokens = self.model.to_tokens(prompt)
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä»˜ãã§ç”Ÿæˆå®Ÿè¡Œ
        with torch.no_grad():
            # ç”Ÿæˆå®Ÿè¡Œ
            generated_tokens = self.model.generate(
                tokens,
                max_new_tokens=self.config.generation.max_new_tokens,
                temperature=self.config.generation.temperature,
                top_p=self.config.generation.top_p,
                top_k=self.config.generation.top_k,
                do_sample=self.config.generation.do_sample,
                repetition_penalty=self.config.generation.repetition_penalty,
                stop_at_eos=True
            )
            
            # ç”Ÿæˆãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒ‡ã‚³ãƒ¼ãƒ‰
            response_text = self.model.to_string(generated_tokens[0])
            
            # SAEæ´»æ€§åŒ–ã‚’å–å¾—ã™ã‚‹ãŸã‚ã€å†åº¦ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰ãƒ‘ã‚¹å®Ÿè¡Œ
            _, cache = self.model.run_with_cache(generated_tokens)
            
            # å¯¾è±¡ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®ãƒ•ãƒƒã‚¯åã‚’å–å¾—
            hook_name = self.sae.cfg.hook_name
            
            # æ´»æ€§åŒ–ã‚’å–å¾—
            activations = cache[hook_name]  # shape: [batch, seq_len, d_model]
            
            # SAEã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
            sae_features = self.sae.encode(activations)  # shape: [batch, seq_len, n_features]
            
            # ãƒˆãƒ¼ã‚¯ãƒ³ä¿å­˜è¨­å®šã«å¿œã˜ã¦å‡¦ç†
            if self.feedback_config.save_all_tokens:
                # å…¨ãƒˆãƒ¼ã‚¯ãƒ³ã®æ´»æ€§åŒ–ã‚’ä¿å­˜
                sae_activations_np = sae_features[0].cpu().numpy()  # [seq_len, n_features]
            else:
                # æœ€å¾Œã®ãƒˆãƒ¼ã‚¯ãƒ³ã®ã¿ä¿å­˜
                sae_activations_np = sae_features[0, -1:].cpu().numpy()  # [1, n_features]
            
            # Top-kç‰¹å¾´ã‚’æŠ½å‡º
            if self.feedback_config.save_all_tokens:
                # å…¨ãƒˆãƒ¼ã‚¯ãƒ³ã®å¹³å‡ã‚’å–ã‚‹
                mean_activations = sae_activations_np.mean(axis=0)
            else:
                mean_activations = sae_activations_np[0]
            
            top_k_indices = np.argsort(mean_activations)[-self.config.analysis.top_k_features:][::-1]
            top_k_features = [(int(idx), float(mean_activations[idx])) for idx in top_k_indices]
            
            # é–¾å€¤ä»¥ä¸Šã®ç‰¹å¾´ã®ã¿ä¿å­˜
            active_features = {}
            threshold = self.config.analysis.activation_threshold
            
            if self.feedback_config.save_all_tokens:
                # å„ãƒˆãƒ¼ã‚¯ãƒ³ä½ç½®ã§ã®æ´»æ€§åŒ–ã‚’ä¿å­˜
                for token_idx in range(sae_activations_np.shape[0]):
                    token_activations = sae_activations_np[token_idx]
                    active_indices = np.where(token_activations > threshold)[0]
                    if len(active_indices) > 0:
                        active_features[f"token_{token_idx}"] = {
                            int(idx): float(token_activations[idx]) 
                            for idx in active_indices
                        }
            else:
                # æœ€å¾Œã®ãƒˆãƒ¼ã‚¯ãƒ³ã®ã¿
                token_activations = sae_activations_np[0]
                active_indices = np.where(token_activations > threshold)[0]
                active_features["last_token"] = {
                    int(idx): float(token_activations[idx]) 
                    for idx in active_indices
                }
            
            sae_info = {
                "hook_name": hook_name,
                "activations": active_features,
                "top_k_features": top_k_features,
                "num_active_features": sum(len(v) for v in active_features.values()),
                "save_all_tokens": self.feedback_config.save_all_tokens,
                "num_tokens": sae_activations_np.shape[0]
            }
        
        return response_text, sae_info
    
    def analyze_prompt_variation(self, prompt_info: FeedbackPromptInfo) -> FeedbackResponse:
        """
        1ã¤ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ã‚’åˆ†æ
        
        Args:
            prompt_info: ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæƒ…å ±
        
        Returns:
            FeedbackResponse ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ
        """
        if self.config.debug.show_prompts:
            print(f"\nğŸ“ Prompt ({prompt_info.prompt_template_type}):")
            print(f"   {prompt_info.prompt[:100]}...")
        
        # ç”Ÿæˆå®Ÿè¡Œ
        start_time = datetime.now()
        response_text, sae_info = self.generate_with_sae(prompt_info.prompt)
        end_time = datetime.now()
        
        if self.config.debug.show_responses:
            print(f"ğŸ’¬ Response:")
            print(f"   {response_text[:200]}...")
        
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
        metadata = {
            "generation_time_ms": (end_time - start_time).total_seconds() * 1000,
            "response_length": len(response_text),
            "timestamp": datetime.now().isoformat()
        }
        
        if torch.cuda.is_available():
            metadata["gpu_memory_mb"] = torch.cuda.memory_allocated() / 1e6
        
        return FeedbackResponse(
            prompt_info=prompt_info,
            response_text=response_text,
            sae_activations=sae_info["activations"],
            top_k_features=sae_info["top_k_features"],
            metadata=metadata
        )
    
    def analyze_question_group(
        self, 
        question_id: int, 
        prompt_group: List[FeedbackPromptInfo]
    ) -> FeedbackQuestionResult:
        """
        1ã¤ã®è³ªå•ï¼ˆ5ã¤ã®ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ï¼‰ã‚’åˆ†æ
        
        Args:
            question_id: è³ªå•ID
            prompt_group: 5ã¤ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³
        
        Returns:
            FeedbackQuestionResult ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ
        """
        if self.config.debug.verbose:
            print(f"\n{'='*60}")
            print(f"ğŸ“Š Analyzing Question {question_id} ({len(prompt_group)} variations)")
            print(f"{'='*60}")
        
        variations_results = []
        
        for prompt_info in prompt_group:
            response = self.analyze_prompt_variation(prompt_info)
            variations_results.append(response)
        
        # æœ€åˆã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‹ã‚‰åŸºæœ¬æƒ…å ±ã‚’å–å¾—
        first_prompt = prompt_group[0]
        base_text = first_prompt.base_data.get('text', '') or first_prompt.base_data.get('question', '')
        
        return FeedbackQuestionResult(
            question_id=question_id,
            dataset=first_prompt.dataset,
            base_text=base_text,
            variations=variations_results,
            timestamp=datetime.now().isoformat()
        )
    
    def run_analysis(self, sample_size: Optional[int] = None):
        """
        å®Œå…¨ãªåˆ†æã‚’å®Ÿè¡Œ
        
        Args:
            sample_size: åˆ†æã™ã‚‹ã‚µãƒ³ãƒ—ãƒ«æ•°ï¼ˆNoneã®å ´åˆã¯configã‹ã‚‰å–å¾—ï¼‰
        """
        if self.config.debug.verbose:
            print("\n" + "="*60)
            print("ğŸš€ Starting Feedback Analysis")
            print("="*60)
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‰
        feedback_data = self.load_feedback_data()
        
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚°ãƒ«ãƒ¼ãƒ—åŒ–
        prompt_groups = self.aggregate_prompts(feedback_data)
        
        # ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºèª¿æ•´
        if sample_size is None:
            sample_size = self.config.data.sample_size
        
        if sample_size is not None and sample_size < len(prompt_groups):
            prompt_groups = prompt_groups[:sample_size]
            if self.config.debug.verbose:
                print(f"ğŸ“Š Analyzing {sample_size} questions (out of {len(prompt_groups)} total)")
        
        # ãƒ¢ãƒ‡ãƒ«ã¨SAEã®ãƒ­ãƒ¼ãƒ‰
        if self.model is None or self.sae is None:
            self.load_model_and_sae()
        
        # å„è³ªå•ã‚°ãƒ«ãƒ¼ãƒ—ã‚’åˆ†æ
        for question_id, prompt_group in enumerate(tqdm(prompt_groups, desc="Analyzing questions")):
            result = self.analyze_question_group(question_id, prompt_group)
            self.results.append(result)
            
            # ãƒ¡ãƒ¢ãƒªã‚¯ãƒªã‚¢
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
        
        if self.config.debug.verbose:
            print("\n" + "="*60)
            print("âœ… Analysis Complete")
            print("="*60)
            print(f"ğŸ“Š Processed {len(self.results)} questions")
            print(f"ğŸ’¾ Total variations: {sum(len(r.variations) for r in self.results)}")
    
    def save_results(self, output_path: Optional[str] = None):
        """
        åˆ†æçµæœã‚’ä¿å­˜
        
        Args:
            output_path: å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ï¼ˆNoneã®å ´åˆã¯è‡ªå‹•ç”Ÿæˆï¼‰
        """
        if not self.results:
            print("âš ï¸ No results to save")
            return
        
        if output_path is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            model_name = self.config.model.name.replace("/", "_")
            output_path = self.results_dir / f"feedback_analysis_{model_name}_{timestamp}.json"
        
        # çµæœã‚’è¾æ›¸ã«å¤‰æ›
        output_data = {
            "metadata": {
                "model_name": self.config.model.name,
                "sae_release": self.config.model.sae_release,
                "sae_id": self.config.model.sae_id,
                "num_questions": len(self.results),
                "save_all_tokens": self.feedback_config.save_all_tokens,
                "target_layer": self.feedback_config.target_layer,
                "timestamp": datetime.now().isoformat(),
                "config": {
                    "sample_size": self.config.data.sample_size,
                    "max_new_tokens": self.config.generation.max_new_tokens,
                    "temperature": self.config.generation.temperature,
                    "top_k_features": self.config.analysis.top_k_features
                }
            },
            "results": []
        }
        
        # å„è³ªå•ã®çµæœã‚’è¿½åŠ 
        for result in self.results:
            question_data = {
                "question_id": result.question_id,
                "dataset": result.dataset,
                "base_text": result.base_text[:200] + "..." if len(result.base_text) > 200 else result.base_text,
                "variations": []
            }
            
            for variation in result.variations:
                variation_data = {
                    "template_type": variation.prompt_info.prompt_template_type,
                    "prompt": variation.prompt_info.prompt if self.config.debug.show_prompts else "[hidden]",
                    "response": variation.response_text,
                    "sae_activations": variation.sae_activations,
                    "top_k_features": variation.top_k_features,
                    "metadata": variation.metadata
                }
                question_data["variations"].append(variation_data)
            
            output_data["results"].append(question_data)
        
        # JSONãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(output_data, f, indent=2, ensure_ascii=False)
        
        if self.config.debug.verbose:
            print(f"\nğŸ’¾ Results saved to: {output_path}")
            file_size = os.path.getsize(output_path) / 1024 / 1024
            print(f"   ğŸ“¦ File size: {file_size:.2f} MB")
    
    def run_complete_analysis(self, sample_size: Optional[int] = None):
        """
        åˆ†æã®å®Ÿè¡Œã¨çµæœä¿å­˜ã‚’ä¸€æ‹¬ã§è¡Œã†
        
        Args:
            sample_size: åˆ†æã™ã‚‹ã‚µãƒ³ãƒ—ãƒ«æ•°
        """
        self.run_analysis(sample_size=sample_size)
        self.save_results()
        
        if self.config.debug.verbose:
            print("\nğŸ‰ Complete analysis finished!")
