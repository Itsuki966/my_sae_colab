{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyN6OGrELDYJg0zLqDIV7pLh"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# ==========================================\n","# 1. ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã¨è¨­å®š\n","# =========================================="],"metadata":{"id":"EGs0qXVxpKEn"}},{"cell_type":"code","source":["# ğŸ”§ ç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ— (Google Drive ãƒã‚¦ãƒ³ãƒˆç‰ˆ)\n","# =================================================\n","import os\n","import sys\n","import torch\n","\n","# 1. Google Driveã‚’ãƒã‚¦ãƒ³ãƒˆ\n","print(\"ğŸ”„ Google Driveã‚’ãƒã‚¦ãƒ³ãƒˆä¸­...\")\n","try:\n","    from google.colab import drive\n","    drive.mount('/content/drive')\n","    print(\"âœ… Google Driveãƒã‚¦ãƒ³ãƒˆå®Œäº†\")\n","except Exception as e:\n","    print(f\"âŒ Google Driveãƒã‚¦ãƒ³ãƒˆã‚¨ãƒ©ãƒ¼: {e}\")\n","    raise\n","\n","# 2. ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ãƒ‘ã‚¹ã‚’è¨­å®š\n","# â€»Google Driveå†…ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ•ã‚©ãƒ«ãƒ€ãƒ‘ã‚¹ã‚’æŒ‡å®šã—ã¦ãã ã•ã„\n","PROJECT_PATH = '/content/drive/MyDrive/sae_pj2'\n","\n","if not os.path.exists(PROJECT_PATH):\n","    print(f\"âŒ ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ•ã‚©ãƒ«ãƒ€ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {PROJECT_PATH}\")\n","    print(\"ğŸ’¡ PROJECT_PATHã®å¤‰æ•°ãŒæ­£ã—ã„ã‹ã€ãƒ•ã‚©ãƒ«ãƒ€ãŒåŒæœŸã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèªã—ã¦ãã ã•ã„ã€‚\")\n","else:\n","    print(f\"âœ… ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ•ã‚©ãƒ«ãƒ€ç™ºè¦‹: {PROJECT_PATH}\")\n","\n","    # 3. PythonãŒãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå†…ã®.pyãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚ã‚‹ã‚ˆã†ã«ãƒ‘ã‚¹ã‚’è¿½åŠ \n","    if PROJECT_PATH not in sys.path:\n","        sys.path.insert(0, PROJECT_PATH)\n","        print(f\"   ğŸ Pythonãƒ‘ã‚¹ã«è¿½åŠ ã—ã¾ã—ãŸ\")\n","\n","    # colabãƒ•ã‚©ãƒ«ãƒ€ã‚‚ãƒ‘ã‚¹ã«è¿½åŠ \n","    colab_path = os.path.join(PROJECT_PATH, 'colab')\n","    if colab_path not in sys.path:\n","        sys.path.insert(0, colab_path)\n","        print(f\"   ğŸ colabãƒ‘ã‚¹ã«è¿½åŠ ã—ã¾ã—ãŸ\")\n","\n","    # 4. ä½œæ¥­ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ•ã‚©ãƒ«ãƒ€ã«ç§»å‹•\n","    os.chdir(PROJECT_PATH)\n","    print(f\"   ğŸ“ ä½œæ¥­ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’å¤‰æ›´ã—ã¾ã—ãŸ: {os.getcwd()}\")\n","\n","    # 4.5 å¿…é ˆãƒ•ã‚¡ã‚¤ãƒ«ã®å­˜åœ¨ãƒã‚§ãƒƒã‚¯\n","    required_items = [\n","        ('config.py', 'è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«'),\n","        ('colab/feedback_analyzer.py', 'ãƒ¡ã‚¤ãƒ³ãƒ­ã‚¸ãƒƒã‚¯'),\n","        ('eval_dataset', 'ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒ•ã‚©ãƒ«ãƒ€'),\n","        ('eval_dataset/feedback.jsonl', 'Feedbackãƒ‡ãƒ¼ã‚¿')\n","    ]\n","    for path, label in required_items:\n","        exists = os.path.exists(path)\n","        icon = 'âœ…' if exists else 'âš ï¸'\n","        kind = 'ğŸ“„' if os.path.isfile(path) else 'ğŸ“'\n","        print(f\"   {icon} å¿…é ˆç¢ºèª: {kind} {path} - {label}{'' if exists else 'ï¼ˆè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯åŒæœŸã‚’ç¢ºèªï¼‰'}\")\n","\n","# 5. ä¾å­˜é–¢ä¿‚ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n","print(\"\\nğŸ“¦ ä¾å­˜é–¢ä¿‚ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ä¸­...\")\n","req_file = 'requirements-colab.txt'\n","if os.path.exists(req_file):\n","    try:\n","        get_ipython().system(f\"pip install -q -r {req_file}\")\n","        print(\"âœ… ä¾å­˜é–¢ä¿‚ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«å®Œäº† (requirements-colab.txtã‚’ä½¿ç”¨)\")\n","    except Exception as e:\n","        print(f\"âš ï¸ ä¾å­˜é–¢ä¿‚ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã§è­¦å‘Š/ã‚¨ãƒ©ãƒ¼: {e}\")\n","        print(\"   å¿…è¦ã«å¿œã˜ã¦å€‹åˆ¥ã« !pip install <package> ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„\")\n","else:\n","    print(f\"âš ï¸ {req_file} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒä¸è¶³ã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚\")\n","\n","# 6. ç’°å¢ƒç¢ºèª\n","print(\"\\nğŸš€ ç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—å®Œäº†ï¼\")\n","if torch.cuda.is_available():\n","    try:\n","        print(f\"   GPUæ¤œå‡º: {torch.cuda.get_device_name(0)}\")\n","        memory_total = torch.cuda.get_device_properties(0).total_memory / 1024**3\n","        print(f\"   GPU Memory: {memory_total:.1f}GB\")\n","    except Exception:\n","        print(\"   GPUæ¤œå‡º: åˆ©ç”¨å¯èƒ½ (ãƒ‡ãƒã‚¤ã‚¹åã®å–å¾—ã«å¤±æ•—)\")\n","else:\n","    print(\"   âŒ GPUåˆ©ç”¨ä¸å¯ (CPUãƒ¢ãƒ¼ãƒ‰)\")\n"],"metadata":{"id":"t-jJR7BrpeUY"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ycxke0Aao38N"},"outputs":[],"source":["# ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–è¨­å®š\n","os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n","torch.set_grad_enabled(True) # å‹¾é…è¨ˆç®—ã‚’æœ‰åŠ¹åŒ–"]},{"cell_type":"code","source":["from huggingface_hub import login\n","login()  # ç”»é¢ã®æŒ‡ç¤ºã«å¾“ã„ãƒˆãƒ¼ã‚¯ãƒ³å…¥åŠ›"],"metadata":{"id":"1oHC_MGwq1bU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# ==========================================\n","# 2. ãƒ‡ãƒ¼ã‚¿å‡¦ç†ãƒ­ã‚¸ãƒƒã‚¯ (æŒ‡ç¤ºæ›¸ã«åŸºã¥ãå®Ÿè£…)\n","# =========================================="],"metadata":{"id":"Qc0KjfyGpT5e"}},{"cell_type":"code","source":["def yield_sycophancy_samples(data: Dict[str, Any]) -> Generator[Dict[str, Any], None, None]:\n","    \"\"\"\n","    JSONãƒ‡ãƒ¼ã‚¿ã‹ã‚‰Attribution Patchingç”¨ã®ã‚µãƒ³ãƒ—ãƒ«ã‚’ç”Ÿæˆã™ã‚‹ã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿\n","\n","    Logic:\n","    1. å„è³ªå•ã«ã¤ã„ã¦ã€ã¾ãš 'base' ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã®å›ç­”ã‚’æ¢ã™ã€‚\n","    2. åŒã˜è³ªå•å†…ã§ sycophancy_flag=1 ã®å›ç­”ã‚’æ¢ã™ã€‚\n","    3. ãƒšã‚¢ãŒè¦‹ã¤ã‹ã‚Œã° yield ã™ã‚‹ã€‚\n","    \"\"\"\n","    results = data.get(\"results\", [])\n","\n","    for result in results:\n","        variations = result.get(\"variations\", [])\n","        question_id = result.get(\"question_id\")\n","\n","        # 1. Baseå›ç­”ã®ç‰¹å®š\n","        base_variation = None\n","        for var in variations:\n","            t_type = var.get(\"template_type\")\n","            # template_typeãŒ 'base' ã¾ãŸã¯ None/ç©ºæ–‡å­— ã®å ´åˆã‚’Baseã¨ã¿ãªã™\n","            if t_type == \"base\" or t_type == \"(base)\" or not t_type:\n","                base_variation = var\n","                break\n","\n","        # BaseãŒå­˜åœ¨ã—ãªã„å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—\n","        if not base_variation:\n","            continue\n","\n","        # 2. ã‚¿ãƒ¼ã‚²ãƒƒãƒˆï¼ˆè¿åˆï¼‰å›ç­”ã®ç‰¹å®šã¨ãƒšã‚¢ãƒªãƒ³ã‚°\n","        for target_variation in variations:\n","            # è‡ªåˆ†è‡ªèº«ï¼ˆBaseï¼‰ã¯ã‚¹ã‚­ãƒƒãƒ—\n","            if target_variation is base_variation:\n","                continue\n","\n","            # sycophancy_flag ãŒ 1 (è¿åˆã—ã¦ã„ã‚‹) ã‚‚ã®ã®ã¿å¯¾è±¡\n","            if target_variation.get(\"sycophancy_flag\") == 1:\n","                yield {\n","                    \"question_id\": question_id,\n","                    \"template_type\": target_variation.get(\"template_type\"),\n","                    \"prompt\": target_variation.get(\"prompt\"),\n","                    \"response\": target_variation.get(\"response\"),       # Target Tokenç”¨\n","                    \"base_response\": base_variation.get(\"response\")     # Base Token (Contrastive) ç”¨\n","                }"],"metadata":{"id":"zUld2F0PpQjB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# ==========================================\n","# 3. Attribution Patching åˆ†æã‚¯ãƒ©ã‚¹\n","# =========================================="],"metadata":{"id":"XgM1PKp8qKJO"}},{"cell_type":"code","source":["class AttributionPatchingAnalyzer:\n","    def __init__(self, model: HookedTransformer, sae: SAE, config: Any):\n","        self.model = model\n","        self.sae = sae\n","        self.config = config\n","        self.hook_name = sae.cfg.hook_name\n","\n","    def _find_answer_start_position(self, full_tokens: torch.Tensor, prompt_str: str) -> int:\n","        \"\"\"\n","        ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®çµ‚ã‚ã‚Šï¼ˆå›ç­”ã®å§‹ã¾ã‚Šï¼‰ã®ãƒˆãƒ¼ã‚¯ãƒ³ä½ç½®ã‚’ç‰¹å®šã™ã‚‹\n","        \"\"\"\n","        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå˜ä½“ã§ã®ãƒˆãƒ¼ã‚¯ãƒ³é•·ã‚’å–å¾—\n","        # Note: BOSãƒˆãƒ¼ã‚¯ãƒ³ç­‰ã®æ‰±ã„ã«æ³¨æ„ã€‚Gemmaã¯add_bos_token=TrueãŒãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ\n","        prompt_tokens = self.model.to_tokens(prompt_str, prepend_bos=True)\n","        return prompt_tokens.shape[1] - 1  # 0-indexed ãªã®ã§ -1 (æœ€å¾Œã®ãƒˆãƒ¼ã‚¯ãƒ³ä½ç½®)\n","\n","    def calculate_atp_for_sample(self, sample: Dict[str, Any]) -> Dict[str, Any]:\n","        \"\"\"\n","        1ã‚µãƒ³ãƒ—ãƒ«ã«å¯¾ã™ã‚‹Attribution Patchingã‚’å®Ÿè¡Œ\n","        \"\"\"\n","        prompt = sample[\"prompt\"]\n","        response = sample[\"response\"]\n","        base_response = sample[\"base_response\"]\n","\n","        # 1. ãƒˆãƒ¼ã‚¯ãƒ³åŒ– (Teacher Forcing Input)\n","        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ + å®Ÿéš›ã®å›ç­”\n","        full_text = prompt + response\n","        input_tokens = self.model.to_tokens(full_text, prepend_bos=True)\n","\n","        # å›ç­”é–‹å§‹ä½ç½®ï¼ˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®æœ€å¾Œã®ãƒˆãƒ¼ã‚¯ãƒ³ä½ç½®ï¼‰ã‚’ç‰¹å®š\n","        # ã“ã“ãŒã€Œæ¬¡ã®ãƒˆãƒ¼ã‚¯ãƒ³ï¼ˆå›ç­”ã®1æ–‡å­—ç›®ï¼‰ã€ã‚’äºˆæ¸¬ã™ã‚‹ä½ç½®ã«ãªã‚‹\n","        target_pos = self._find_answer_start_position(input_tokens, prompt)\n","\n","        # å…¥åŠ›é•·ãƒã‚§ãƒƒã‚¯ (ãƒ¢ãƒ‡ãƒ«ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆé•·ã‚’è¶…ãˆãªã„ã‹)\n","        if input_tokens.shape[1] > self.model.cfg.n_ctx:\n","            return {\"error\": \"Sequence too long\"}\n","\n","        # 2. Target Token ã¨ Base Token ã® ID ã‚’å–å¾—\n","        # å®Ÿéš›ã«ç”Ÿæˆã•ã‚ŒãŸå›ç­”ã®1ãƒˆãƒ¼ã‚¯ãƒ³ç›®\n","        target_token_id = input_tokens[0, target_pos + 1].item()\n","\n","        # Baseå›ç­”ã®1ãƒˆãƒ¼ã‚¯ãƒ³ç›® (Contrastive Pairs)\n","        # Baseå›ç­”ã‚’ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã—ã¦æœ€åˆã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚’å–å¾—\n","        # Prompt + Base Response ã§ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã—ãªã„ã¨ã€çµåˆéƒ¨ã®ãƒˆãƒ¼ã‚¯ãƒ³ãŒå¤‰ã‚ã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ãŸã‚æ³¨æ„\n","        base_full_text = prompt + base_response\n","        base_input_tokens = self.model.to_tokens(base_full_text, prepend_bos=True)\n","\n","        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆé•·ãŒåŒã˜ã§ã‚ã‚‹ã¨ä»®å®šï¼ˆåŒã˜ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãªã®ã§ï¼‰\n","        # ãŸã ã—ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ã®æŒ™å‹•ã«ã‚ˆã‚Šã‚ºãƒ¬ã‚‹å¯èƒ½æ€§ã‚‚ã‚¼ãƒ­ã§ã¯ãªã„ãŸã‚ã€å®‰å…¨ç­–ã‚’ã¨ã‚‹ãªã‚‰ã“ã“ã§Baseå´ã®posã‚‚å†è¨ˆç®—æ¨å¥¨\n","        # ä»Šå›ã¯ç°¡ç•¥åŒ–ã®ãŸã‚ã€PromptãŒåŒä¸€ãªã‚‰é•·ã•ã‚‚åŒä¸€ã¨ã¿ãªã™\n","        try:\n","            base_token_id = base_input_tokens[0, target_pos + 1].item()\n","        except IndexError:\n","            # Baseå›ç­”ãŒç©ºã€ã‚ã‚‹ã„ã¯æ¥µç«¯ã«çŸ­ã„å ´åˆ\n","            return {\"error\": \"Base response too short\"}\n","\n","        # Targetã¨BaseãŒåŒã˜ãƒˆãƒ¼ã‚¯ãƒ³ãªã‚‰ã€Logit Diffã¯0ã«ãªã‚Šå‹¾é…ã‚‚0ãªã®ã§ã‚¹ã‚­ãƒƒãƒ—å¯èƒ½\n","        if target_token_id == base_token_id:\n","            return {\"skipped\": \"Target and Base tokens are identical\"}\n","\n","        # 3. Forward Pass & Metric Calculation\n","        self.model.eval()\n","        self.model.zero_grad()\n","\n","        # ãƒ•ãƒƒã‚¯å†…ã§ãƒ‡ãƒ¼ã‚¿ã‚’ã‚­ãƒ£ãƒ—ãƒãƒ£ã™ã‚‹ãŸã‚ã®ã‚³ãƒ³ãƒ†ãƒŠ\n","        feature_acts_storage = {}\n","\n","        def atp_hook(activation, hook):\n","            \"\"\"\n","            Activationã‚’å–å¾—ã—ã€SAEã‚’é€šã—ã¦å‹¾é…ã‚’æµã™ãƒ•ãƒƒã‚¯\n","            \"\"\"\n","            # activation: [batch, seq, d_model]\n","            # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆä½ç½®ï¼ˆå›ç­”ç›´å‰ï¼‰ã®ã¿ã‚’æŠ½å‡º\n","            # batch=1 å‰æ\n","            target_act = activation[:, target_pos:target_pos+1, :]\n","\n","            # SAE Encode (Feature Activationè¨ˆç®—)\n","            # SAEã®å…¥åŠ›æ¬¡å…ƒã«åˆã‚ã›ã¦èª¿æ•´\n","            f_acts = self.sae.encode(target_act) # [1, 1, n_features]\n","\n","            # å‹¾é…è¨ˆç®—ã®ãŸã‚ã«ä¿å­˜ (retain_gradé‡è¦)\n","            f_acts.requires_grad_(True)\n","            f_acts.retain_grad()\n","            feature_acts_storage['acts'] = f_acts\n","\n","            # SAE Decode (Reconstruction)\n","            x_hat = self.sae.decode(f_acts)\n","\n","            # Gradient Trick:\n","            # Forward: å…ƒã®Activation (x) ã‚’ãã®ã¾ã¾æµã™ (Teacher Forcingã®ç²¾åº¦ç¶­æŒ)\n","            # Backward: Reconstruction (x_hat) ã‚’é€šã—ã¦å‹¾é…ã‚’æµã™ (SAEç‰¹å¾´é‡ã¸ã®Pathã‚’ä½œã‚‹)\n","            # x_out = x_hat + (x - x_hat).detach()\n","            # ã“ã‚Œã«ã‚ˆã‚Šã€Metricã®å‹¾é…ã¯ x_hat -> f_acts ã¨ä¼æ’­ã™ã‚‹\n","\n","            x_out = x_hat + (target_act - x_hat).detach()\n","\n","            # å…ƒã®ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã«æˆ»ã™\n","            activation[:, target_pos:target_pos+1, :] = x_out\n","            return activation\n","\n","        # ãƒ¢ãƒ‡ãƒ«å®Ÿè¡Œ\n","        try:\n","            logits = self.model.run_with_hooks(\n","                input_tokens,\n","                fwd_hooks=[(self.hook_name, atp_hook)]\n","            )\n","\n","            # 4. Metric Calculation (Logit Difference)\n","            # target_posã®ä½ç½®ã§ã®äºˆæ¸¬ã‚’è¦‹ã‚‹\n","            target_logit = logits[0, target_pos, target_token_id]\n","            base_logit = logits[0, target_pos, base_token_id]\n","            metric = target_logit - base_logit\n","\n","            # 5. Backward Pass\n","            metric.backward()\n","\n","            # 6. AtP Score Calculation\n","            # Score = Activation * Gradient\n","            f_acts = feature_acts_storage['acts']\n","            f_grad = f_acts.grad\n","\n","            if f_acts is None or f_grad is None:\n","                return {\"error\": \"Failed to capture gradients\"}\n","\n","            atp_scores = (f_acts * f_grad).detach().cpu().squeeze() # [n_features]\n","\n","            # çµæœã®æŠ½å‡ºï¼ˆTop-K & Non-zeroï¼‰\n","            # ãƒ¡ãƒ¢ãƒªç¯€ç´„ã®ãŸã‚ã€ã‚¹ã‚³ã‚¢ãŒé«˜ã„ã‚‚ã®ã ã‘ã‚’ä¿å­˜\n","            top_k = 50\n","            top_indices = torch.topk(atp_scores.abs(), k=top_k).indices\n","\n","            result_scores = {}\n","            for idx in top_indices:\n","                idx_val = idx.item()\n","                score = atp_scores[idx_val].item()\n","                activation_val = f_acts[0, 0, idx_val].item()\n","\n","                # æ´»æ€§åŒ–ã—ã¦ã„ãªã„ï¼ˆ0ã¾ãŸã¯éå¸¸ã«å°ã•ã„ï¼‰ç‰¹å¾´ã¯å› æœåŠ¹æœãŒãªã„ãŸã‚é™¤å¤–ã—ã¦ã‚‚ã‚ˆã„ãŒ\n","                # è² ã®å‹¾é…ï¼ˆæŠ‘åˆ¶ã™ã¹ãã ã£ãŸã®ã«ç™ºç«ã—ãŸç­‰ï¼‰ã‚‚é‡è¦ãªã®ã§ã€AtPã‚¹ã‚³ã‚¢çµ¶å¯¾å€¤ã§ä¿å­˜\n","                result_scores[str(idx_val)] = {\n","                    \"atp_score\": score,\n","                    \"activation\": activation_val,\n","                    \"gradient\": f_grad[0, 0, idx_val].item()\n","                }\n","\n","            return {\n","                \"status\": \"success\",\n","                \"target_token\": self.model.to_string(target_token_id),\n","                \"base_token\": self.model.to_string(base_token_id),\n","                \"logit_diff\": metric.item(),\n","                \"atp_scores\": result_scores\n","            }\n","\n","        except RuntimeError as e:\n","            if \"out of memory\" in str(e).lower():\n","                torch.cuda.empty_cache()\n","                return {\"error\": \"OOM\"}\n","            raise e\n","        finally:\n","            # ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—\n","            self.model.zero_grad()\n","            del feature_acts_storage\n","            if 'logits' in locals(): del logits\n","            if 'f_acts' in locals(): del f_acts\n","            if 'f_grad' in locals(): del f_grad\n","            torch.cuda.empty_cache()"],"metadata":{"id":"NZ7TPol3qHMC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# ==========================================\n","# 4. ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œã‚¹ã‚¯ãƒªãƒ—ãƒˆ\n","# =========================================="],"metadata":{"id":"tEYEbjxRqP-R"}},{"cell_type":"code","source":["def run_attribution_patching_pipeline():\n","    # ãƒ‘ã‚¹è¨­å®š\n","    PROJECT_PATH = '/content/drive/MyDrive/sae_pj2'\n","    INPUT_JSON_PATH = os.path.join(PROJECT_PATH, \"results/feedback/feedback_analysis_gemma-2-9b-it_20251117_0-100_labeled.json\") # â€»ãƒ•ã‚¡ã‚¤ãƒ«åã¯é©å®œå¤‰æ›´ã—ã¦ãã ã•ã„\n","    OUTPUT_JSON_PATH = os.path.join(PROJECT_PATH, \"results/feedback/atp_results_gemma-2-9b-it.json\")\n","\n","    # ãƒ•ã‚¡ã‚¤ãƒ«å­˜åœ¨ç¢ºèª\n","    if not os.path.exists(INPUT_JSON_PATH):\n","        # ãƒ†ã‚¹ãƒˆç”¨ã«æœ€æ–°ã®çµæœãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¢ã™\n","        search_dir = Path(os.path.join(PROJECT_PATH, \"results/feedback\"))\n","        files = list(search_dir.glob(\"feedback_analysis_*.json\"))\n","        if files:\n","            INPUT_JSON_PATH = str(sorted(files)[-1])\n","            print(f\"âš ï¸ æŒ‡å®šã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚‰ãªã„ãŸã‚ã€æœ€æ–°ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ç”¨ã—ã¾ã™: {INPUT_JSON_PATH}\")\n","        else:\n","            raise FileNotFoundError(\"Input JSON file not found.\")\n","\n","    # 1. ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿\n","    print(f\"ğŸ“‚ Loading data from {INPUT_JSON_PATH}...\")\n","    with open(INPUT_JSON_PATH, 'r', encoding='utf-8') as f:\n","        data = json.load(f)\n","\n","    # 2. ãƒ¢ãƒ‡ãƒ«ã¨SAEã®æº–å‚™\n","    # Note: æ—¢ã«ãƒ¡ãƒ¢ãƒªã«ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¦ã„ã‚‹å ´åˆã¯å†åˆ©ç”¨å¯èƒ½ã§ã™ãŒã€ã“ã“ã§ã¯å®‰å…¨ã®ãŸã‚configã‹ã‚‰ãƒ­ãƒ¼ãƒ‰ã‚’æƒ³å®š\n","    from config import FEEDBACK_GEMMA2_9B_IT_CONFIG\n","\n","    # ãƒ¡ãƒ¢ãƒªä¸è¶³å¯¾ç­–: batch_size=1, gradient checkpointingãªã©ã¯ç„¡åŠ¹åŒ–ï¼ˆæ¨è«–ãƒ¢ãƒ¼ãƒ‰ãªã®ã§ï¼‰\n","    print(\"ğŸ”„ Loading Model & SAE...\")\n","    # æ—¢å­˜ã®analyzerãŒã‚ã‚Œã°ãã‚Œã‚’ä½¿ã†ã€ãªã‘ã‚Œã°æ–°è¦ä½œæˆ\n","    # ã“ã“ã§ã¯ç°¡æ˜“çš„ã«ç›´æ›¸ãã—ã¾ã™\n","    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","\n","    model = HookedTransformer.from_pretrained_no_processing(\n","        FEEDBACK_GEMMA2_9B_IT_CONFIG.model.name,\n","        device=device,\n","        dtype=torch.bfloat16\n","    )\n","\n","    sae, _, _ = SAE.from_pretrained(\n","        release=FEEDBACK_GEMMA2_9B_IT_CONFIG.model.sae_release,\n","        sae_id=FEEDBACK_GEMMA2_9B_IT_CONFIG.model.sae_id,\n","        device=device\n","    )\n","\n","    analyzer = AttributionPatchingAnalyzer(model, sae, FEEDBACK_GEMMA2_9B_IT_CONFIG)\n","\n","    # 3. ãƒ¡ã‚¤ãƒ³ãƒ«ãƒ¼ãƒ—\n","    results = []\n","    samples_gen = yield_sycophancy_samples(data)\n","\n","    # ã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿ã‚’ãƒªã‚¹ãƒˆåŒ–ã—ã¦tqdmã§å›ã™ï¼ˆãƒ¡ãƒ¢ãƒªæ³¨æ„ï¼‰\n","    # å¤§é‡ãƒ‡ãƒ¼ã‚¿ã®å ´åˆã¯ãƒãƒƒãƒå‡¦ç†æ¨å¥¨\n","    samples = list(samples_gen)\n","    print(f\"ğŸš€ Starting ATP analysis for {len(samples)} samples...\")\n","\n","    for i, sample in enumerate(tqdm(samples)):\n","        res = analyzer.calculate_atp_for_sample(sample)\n","\n","        # çµæœã‚’ã‚µãƒ³ãƒ—ãƒ«æƒ…å ±ã«çµåˆ\n","        sample_result = {\n","            \"question_id\": sample[\"question_id\"],\n","            \"template_type\": sample[\"template_type\"],\n","            \"atp_analysis\": res\n","        }\n","        results.append(sample_result)\n","\n","        # å®šæœŸçš„ã«ä¿å­˜ï¼ˆã‚¯ãƒ©ãƒƒã‚·ãƒ¥å¯¾ç­–ï¼‰\n","        if (i + 1) % 10 == 0:\n","            with open(OUTPUT_JSON_PATH, 'w', encoding='utf-8') as f:\n","                json.dump({\"results\": results}, f, indent=2, ensure_ascii=False)\n","            # ã‚¬ãƒ™ãƒ¼ã‚¸ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³\n","            gc.collect()\n","            torch.cuda.empty_cache()\n","\n","    # 4. æœ€çµ‚ä¿å­˜\n","    with open(OUTPUT_JSON_PATH, 'w', encoding='utf-8') as f:\n","        json.dump({\"results\": results}, f, indent=2, ensure_ascii=False)\n","\n","    print(f\"âœ… Analysis completed. Saved to {OUTPUT_JSON_PATH}\")"],"metadata":{"id":"--Ye7DiWqSZH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# ==========================================\n","# 4. å®Ÿè¡Œ\n","# ==========================================\n"],"metadata":{"id":"kAPtlItTqYiT"}},{"cell_type":"code","source":["# å®Ÿè¡Œ\n","if __name__ == \"__main__\":\n","    # Colabç’°å¢ƒã§ã®å®Ÿè¡Œã‚’æƒ³å®š\n","    try:\n","        run_attribution_patching_pipeline()\n","    except Exception as e:\n","        print(f\"âŒ Error during execution: {e}\")\n","        import traceback\n","        traceback.print_exc()"],"metadata":{"id":"MXu0pcMiqclK"},"execution_count":null,"outputs":[]}]}