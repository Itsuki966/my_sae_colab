{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EGs0qXVxpKEn"
   },
   "source": [
    "# ==========================================\n",
    "# 1. ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã¨è¨­å®š\n",
    "# =========================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9316,
     "status": "ok",
     "timestamp": 1765896835581,
     "user": {
      "displayName": "Kuwahara Itsuki",
      "userId": "08552036371601467574"
     },
     "user_tz": -540
    },
    "id": "t-jJR7BrpeUY",
    "outputId": "a8c8e7ae-ff58-4579-b313-490dd355743c"
   },
   "outputs": [],
   "source": [
    "# ğŸ”§ ç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ— (Google Drive ãƒã‚¦ãƒ³ãƒˆç‰ˆ)\n",
    "# =================================================\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "\n",
    "# 1. Google Driveã‚’ãƒã‚¦ãƒ³ãƒˆ\n",
    "print(\"ğŸ”„ Google Driveã‚’ãƒã‚¦ãƒ³ãƒˆä¸­...\")\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    print(\"âœ… Google Driveãƒã‚¦ãƒ³ãƒˆå®Œäº†\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Google Driveãƒã‚¦ãƒ³ãƒˆã‚¨ãƒ©ãƒ¼: {e}\")\n",
    "    raise\n",
    "\n",
    "# 2. ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ãƒ‘ã‚¹ã‚’è¨­å®š\n",
    "# â€»Google Driveå†…ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ•ã‚©ãƒ«ãƒ€ãƒ‘ã‚¹ã‚’æŒ‡å®šã—ã¦ãã ã•ã„\n",
    "PROJECT_PATH = '/content/drive/MyDrive/sae_pj2'\n",
    "\n",
    "if not os.path.exists(PROJECT_PATH):\n",
    "    print(f\"âŒ ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ•ã‚©ãƒ«ãƒ€ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {PROJECT_PATH}\")\n",
    "    print(\"ğŸ’¡ PROJECT_PATHã®å¤‰æ•°ãŒæ­£ã—ã„ã‹ã€ãƒ•ã‚©ãƒ«ãƒ€ãŒåŒæœŸã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèªã—ã¦ãã ã•ã„ã€‚\")\n",
    "else:\n",
    "    print(f\"âœ… ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ•ã‚©ãƒ«ãƒ€ç™ºè¦‹: {PROJECT_PATH}\")\n",
    "\n",
    "    # 3. PythonãŒãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå†…ã®.pyãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚ã‚‹ã‚ˆã†ã«ãƒ‘ã‚¹ã‚’è¿½åŠ \n",
    "    if PROJECT_PATH not in sys.path:\n",
    "        sys.path.insert(0, PROJECT_PATH)\n",
    "        print(f\"   ğŸ Pythonãƒ‘ã‚¹ã«è¿½åŠ ã—ã¾ã—ãŸ\")\n",
    "\n",
    "    # colabãƒ•ã‚©ãƒ«ãƒ€ã‚‚ãƒ‘ã‚¹ã«è¿½åŠ \n",
    "    colab_path = os.path.join(PROJECT_PATH, 'colab')\n",
    "    if colab_path not in sys.path:\n",
    "        sys.path.insert(0, colab_path)\n",
    "        print(f\"   ğŸ colabãƒ‘ã‚¹ã«è¿½åŠ ã—ã¾ã—ãŸ\")\n",
    "\n",
    "    # 4. ä½œæ¥­ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ•ã‚©ãƒ«ãƒ€ã«ç§»å‹•\n",
    "    os.chdir(PROJECT_PATH)\n",
    "    print(f\"   ğŸ“ ä½œæ¥­ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’å¤‰æ›´ã—ã¾ã—ãŸ: {os.getcwd()}\")\n",
    "\n",
    "    # 4.5 å¿…é ˆãƒ•ã‚¡ã‚¤ãƒ«ã®å­˜åœ¨ãƒã‚§ãƒƒã‚¯\n",
    "    required_items = [\n",
    "        ('config.py', 'è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«'),\n",
    "        ('colab/feedback_analyzer.py', 'ãƒ¡ã‚¤ãƒ³ãƒ­ã‚¸ãƒƒã‚¯'),\n",
    "        ('eval_dataset', 'ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒ•ã‚©ãƒ«ãƒ€'),\n",
    "        ('eval_dataset/feedback.jsonl', 'Feedbackãƒ‡ãƒ¼ã‚¿')\n",
    "    ]\n",
    "    for path, label in required_items:\n",
    "        exists = os.path.exists(path)\n",
    "        icon = 'âœ…' if exists else 'âš ï¸'\n",
    "        kind = 'ğŸ“„' if os.path.isfile(path) else 'ğŸ“'\n",
    "        print(f\"   {icon} å¿…é ˆç¢ºèª: {kind} {path} - {label}{'' if exists else 'ï¼ˆè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯åŒæœŸã‚’ç¢ºèªï¼‰'}\")\n",
    "\n",
    "# 5. ä¾å­˜é–¢ä¿‚ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n",
    "print(\"\\nğŸ“¦ ä¾å­˜é–¢ä¿‚ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ä¸­...\")\n",
    "req_file = 'requirements-colab.txt'\n",
    "if os.path.exists(req_file):\n",
    "    try:\n",
    "        get_ipython().system(f\"pip install -q -r {req_file}\")\n",
    "        print(\"âœ… ä¾å­˜é–¢ä¿‚ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«å®Œäº† (requirements-colab.txtã‚’ä½¿ç”¨)\")\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ ä¾å­˜é–¢ä¿‚ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã§è­¦å‘Š/ã‚¨ãƒ©ãƒ¼: {e}\")\n",
    "        print(\"   å¿…è¦ã«å¿œã˜ã¦å€‹åˆ¥ã« !pip install <package> ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„\")\n",
    "else:\n",
    "    print(f\"âš ï¸ {req_file} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒä¸è¶³ã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚\")\n",
    "\n",
    "# 6. ç’°å¢ƒç¢ºèª\n",
    "print(\"\\nğŸš€ ç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—å®Œäº†ï¼\")\n",
    "if torch.cuda.is_available():\n",
    "    try:\n",
    "        print(f\"   GPUæ¤œå‡º: {torch.cuda.get_device_name(0)}\")\n",
    "        memory_total = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "        print(f\"   GPU Memory: {memory_total:.1f}GB\")\n",
    "    except Exception:\n",
    "        print(\"   GPUæ¤œå‡º: åˆ©ç”¨å¯èƒ½ (ãƒ‡ãƒã‚¤ã‚¹åã®å–å¾—ã«å¤±æ•—)\")\n",
    "else:\n",
    "    print(\"   âŒ GPUåˆ©ç”¨ä¸å¯ (CPUãƒ¢ãƒ¼ãƒ‰)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3212,
     "status": "ok",
     "timestamp": 1765896838807,
     "user": {
      "displayName": "Kuwahara Itsuki",
      "userId": "08552036371601467574"
     },
     "user_tz": -540
    },
    "id": "Ycxke0Aao38N",
    "outputId": "79cdf220-5aeb-479c-9de0-305a530f5b1d"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import gc\n",
    "from typing import Dict, Any, Generator\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from transformer_lens import HookedTransformer\n",
    "from sae_lens import SAE\n",
    "\n",
    "# ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–è¨­å®š\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "torch.set_grad_enabled(True) # å‹¾é…è¨ˆç®—ã‚’æœ‰åŠ¹åŒ–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17,
     "referenced_widgets": [
      "8a9ffec5f8484b4195edcfdbe964046e",
      "32e298d4dc404fd4a6518da56b3ec839",
      "3dc871af7cef4277913a24cca68bbcf3",
      "c387fc8f99c841d08e8161848ae06c74",
      "f3e319f521e841e0983d9c0664424def",
      "6180f6670490493c89186793d038c996",
      "443fdb2fc93342dfbabbb55c7f1e0f01",
      "3b8a438247c249c69952d326f51eb97a",
      "f9152bb3863840ef9a00fe2aaeaebf40",
      "3eae439ff8954cdc9d305c8ec0fbc8d4",
      "0f63b4e1786948109185728b22d2e25e",
      "40e5bc5bc2d54363be268930c8671d50",
      "6aa3e365a24b4ce29b07ef7dcd39611e",
      "b0dd67f7c11d4626a9b960a437904168",
      "74162eea75ac4138bbb8b7144a806f4d",
      "9037406cb747438f8b7a3baad0649eb2",
      "60c27a47951a49ef9c02ee6678507744",
      "bce6a0cd6bc54bfab26c399142d4b5ea",
      "927b578a4aa04571b4bcf70556facac0",
      "e8d99475cdbd4967b8ee478d74cfc772"
     ]
    },
    "executionInfo": {
     "elapsed": 71,
     "status": "ok",
     "timestamp": 1765894631434,
     "user": {
      "displayName": "Kuwahara Itsuki",
      "userId": "08552036371601467574"
     },
     "user_tz": -540
    },
    "id": "1oHC_MGwq1bU",
    "outputId": "689a1b47-454b-4e99-8fea-c33dd1b23514"
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login()  # ç”»é¢ã®æŒ‡ç¤ºã«å¾“ã„ãƒˆãƒ¼ã‚¯ãƒ³å…¥åŠ›"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qc0KjfyGpT5e"
   },
   "source": [
    "# ==========================================\n",
    "# 2. ãƒ‡ãƒ¼ã‚¿å‡¦ç†ãƒ­ã‚¸ãƒƒã‚¯ (æŒ‡ç¤ºæ›¸ã«åŸºã¥ãå®Ÿè£…)\n",
    "# =========================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 43,
     "status": "ok",
     "timestamp": 1765896932322,
     "user": {
      "displayName": "Kuwahara Itsuki",
      "userId": "08552036371601467574"
     },
     "user_tz": -540
    },
    "id": "zUld2F0PpQjB"
   },
   "outputs": [],
   "source": [
    "def yield_sycophancy_samples(data: Dict[str, Any]) -> Generator[Dict[str, Any], None, None]:\n",
    "    \"\"\"\n",
    "    JSONãƒ‡ãƒ¼ã‚¿ã‹ã‚‰Attribution Patchingç”¨ã®ã‚µãƒ³ãƒ—ãƒ«ã‚’ç”Ÿæˆã™ã‚‹ã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿\n",
    "    \n",
    "    æ–°æ—§ä¸¡æ–¹ã®ãƒ•ã‚¡ã‚¤ãƒ«æ§‹é€ ã«å¯¾å¿œ:\n",
    "    - æ—§å½¢å¼: variationsç›´ä¸‹ã«prompt/responseãŒã‚ã‚‹\n",
    "    - æ–°å½¢å¼: metadataã‚»ã‚¯ã‚·ãƒ§ãƒ³ãŒã‚ã‚Šã€sae_activationsãªã©æ–°æ§‹é€ ï¼ˆtemplate_type=\"\"ãŒbaseï¼‰\n",
    "    \"\"\"\n",
    "    results = data.get(\"results\", [])\n",
    "\n",
    "    for result in results:\n",
    "        variations = result.get(\"variations\", [])\n",
    "        question_id = result.get(\"question_id\")\n",
    "\n",
    "        # 1. Baseå›ç­”ã®ç‰¹å®šï¼ˆç©ºæ–‡å­—åˆ—ã‚‚baseã¨ã¿ãªã™ï¼‰\n",
    "        base_variation = None\n",
    "        for var in variations:\n",
    "            t_type = var.get(\"template_type\")\n",
    "            if t_type == \"base\" or t_type == \"(base)\" or t_type == \"\" or not t_type:\n",
    "                base_variation = var\n",
    "                break\n",
    "\n",
    "        if not base_variation:\n",
    "            continue\n",
    "\n",
    "        # 2. ã‚¿ãƒ¼ã‚²ãƒƒãƒˆï¼ˆè¿åˆï¼‰å›ç­”ã®ç‰¹å®šã¨ãƒšã‚¢ãƒªãƒ³ã‚°\n",
    "        for idx, target_variation in enumerate(variations):\n",
    "            if target_variation is base_variation:\n",
    "                continue\n",
    "\n",
    "            if target_variation.get(\"sycophancy_flag\") == 1:\n",
    "                yield {\n",
    "                    \"question_id\": question_id,\n",
    "                    \"variation_index\": idx,\n",
    "                    \"template_type\": target_variation.get(\"template_type\"),\n",
    "                    \"prompt\": target_variation.get(\"prompt\"),\n",
    "                    \"target_response\": target_variation.get(\"response\"),\n",
    "                    \"base_response\": base_variation.get(\"response\")\n",
    "                }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XgM1PKp8qKJO"
   },
   "source": [
    "# ==========================================\n",
    "# 3. Attribution Patching åˆ†æã‚¯ãƒ©ã‚¹\n",
    "# =========================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1765896933798,
     "user": {
      "displayName": "Kuwahara Itsuki",
      "userId": "08552036371601467574"
     },
     "user_tz": -540
    },
    "id": "NZ7TPol3qHMC"
   },
   "outputs": [],
   "source": [
    "class AttributionPatchingAnalyzer:\n",
    "    def __init__(self, model: HookedTransformer, sae: SAE, config: Any):\n",
    "        self.model = model\n",
    "        self.sae = sae\n",
    "        self.config = config\n",
    "        # hook_name ã‚’ç›´æ¥æŒ‡å®šï¼ˆconfig.py ã‹ã‚‰å–å¾—ã™ã‚‹ã®ãŒç†æƒ³ï¼‰\n",
    "        # Gemma Scope SAE ã® sae_id ã‹ã‚‰æ¨å®š\n",
    "        # ä¾‹: \"layer_31/width_16k/canonical\" â†’ \"blocks.31.hook_resid_post\"\n",
    "        sae_id = config.model.sae_id\n",
    "        layer_num = sae_id.split('/')[0].replace('layer_', '')\n",
    "        self.hook_name = f\"blocks.{layer_num}.hook_resid_post\"\n",
    "        print(f\"   ğŸ¯ Using hook: {self.hook_name}\")\n",
    "\n",
    "    def _find_answer_start_position(self, full_tokens: torch.Tensor, prompt_str: str) -> int:\n",
    "        \"\"\"\n",
    "        ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®çµ‚ã‚ã‚Šï¼ˆå›ç­”ã®å§‹ã¾ã‚Šï¼‰ã®ãƒˆãƒ¼ã‚¯ãƒ³ä½ç½®ã‚’ç‰¹å®šã™ã‚‹\n",
    "        \"\"\"\n",
    "        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå˜ä½“ã§ã®ãƒˆãƒ¼ã‚¯ãƒ³é•·ã‚’å–å¾—\n",
    "        # Note: BOSãƒˆãƒ¼ã‚¯ãƒ³ç­‰ã®æ‰±ã„ã«æ³¨æ„ã€‚Gemmaã¯add_bos_token=TrueãŒãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ\n",
    "        prompt_tokens = self.model.to_tokens(prompt_str, prepend_bos=True)\n",
    "        return prompt_tokens.shape[1] - 1  # 0-indexed ãªã®ã§ -1 (æœ€å¾Œã®ãƒˆãƒ¼ã‚¯ãƒ³ä½ç½®)\n",
    "\n",
    "    def calculate_atp_for_sample(self, sample: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        1ã‚µãƒ³ãƒ—ãƒ«ã«å¯¾ã™ã‚‹Attribution Patchingã‚’å®Ÿè¡Œ\n",
    "        \"\"\"\n",
    "        prompt = sample[\"prompt\"]\n",
    "        response = sample[\"target_response\"]\n",
    "        base_response = sample[\"base_response\"]\n",
    "\n",
    "        # 1. ãƒˆãƒ¼ã‚¯ãƒ³åŒ– (Teacher Forcing Input)\n",
    "        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ + å®Ÿéš›ã®å›ç­”\n",
    "        full_text = prompt + response\n",
    "        input_tokens = self.model.to_tokens(full_text, prepend_bos=True)\n",
    "\n",
    "        # å›ç­”é–‹å§‹ä½ç½®ï¼ˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®æœ€å¾Œã®ãƒˆãƒ¼ã‚¯ãƒ³ä½ç½®ï¼‰ã‚’ç‰¹å®š\n",
    "        # ã“ã“ãŒã€Œæ¬¡ã®ãƒˆãƒ¼ã‚¯ãƒ³ï¼ˆå›ç­”ã®1æ–‡å­—ç›®ï¼‰ã€ã‚’äºˆæ¸¬ã™ã‚‹ä½ç½®ã«ãªã‚‹\n",
    "        target_pos = self._find_answer_start_position(input_tokens, prompt)\n",
    "\n",
    "        # å…¥åŠ›é•·ãƒã‚§ãƒƒã‚¯ (ãƒ¢ãƒ‡ãƒ«ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆé•·ã‚’è¶…ãˆãªã„ã‹)\n",
    "        if input_tokens.shape[1] > self.model.cfg.n_ctx:\n",
    "            return {\"error\": \"Sequence too long\"}\n",
    "\n",
    "        # 2. Target Token ã¨ Base Token ã® ID ã‚’å–å¾—\n",
    "        # Baseå›ç­”ã‚’ãƒˆãƒ¼ã‚¯ãƒ³åŒ–\n",
    "        base_full_text = prompt + base_response\n",
    "        base_input_tokens = self.model.to_tokens(base_full_text, prepend_bos=True)\n",
    "        base_target_pos = self._find_answer_start_position(base_input_tokens, prompt)\n",
    "\n",
    "        # è¤‡æ•°ãƒˆãƒ¼ã‚¯ãƒ³ä½ç½®ã‚’è©¦ã—ã¦ã€ç•°ãªã‚‹ãƒˆãƒ¼ã‚¯ãƒ³ãƒšã‚¢ã‚’è¦‹ã¤ã‘ã‚‹\n",
    "        max_tokens_to_check = 20  # æœ€å¤§20ãƒˆãƒ¼ã‚¯ãƒ³ã¾ã§ãƒã‚§ãƒƒã‚¯\n",
    "        target_token_id = None\n",
    "        base_token_id = None\n",
    "        token_offset = 1  # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç›´å¾Œã‹ã‚‰ã‚¹ã‚¿ãƒ¼ãƒˆ\n",
    "\n",
    "        for offset in range(1, max_tokens_to_check + 1):\n",
    "            try:\n",
    "                candidate_target = input_tokens[0, target_pos + offset].item()\n",
    "                candidate_base = base_input_tokens[0, base_target_pos + offset].item()\n",
    "\n",
    "                # ç•°ãªã‚‹ãƒˆãƒ¼ã‚¯ãƒ³ãŒè¦‹ã¤ã‹ã£ãŸã‚‰æ¡ç”¨\n",
    "                if candidate_target != candidate_base:\n",
    "                    target_token_id = candidate_target\n",
    "                    base_token_id = candidate_base\n",
    "                    token_offset = offset\n",
    "                    break\n",
    "            except IndexError:\n",
    "                # ã©ã¡ã‚‰ã‹ã®å›ç­”ãŒçŸ­ã™ãã‚‹å ´åˆ\n",
    "                break\n",
    "\n",
    "        # ã™ã¹ã¦ã®ãƒˆãƒ¼ã‚¯ãƒ³ãŒåŒä¸€ã€ã¾ãŸã¯å–å¾—å¤±æ•—ã®å ´åˆ\n",
    "        if target_token_id is None or base_token_id is None:\n",
    "            return {\"skipped\": \"No differing tokens found in first 5 positions\"}\n",
    "\n",
    "        # ä½¿ç”¨ã™ã‚‹ãƒˆãƒ¼ã‚¯ãƒ³ä½ç½®ã‚’æ›´æ–°ï¼ˆLogitå–å¾—ç”¨ï¼‰\n",
    "        # target_pos ã¯ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæœ€å¾Œã®ä½ç½®ãªã®ã§ã€offset-1 ã®ä½ç½®ã®Logitã‚’è¦‹ã‚‹\n",
    "        logit_pos = target_pos + token_offset - 1\n",
    "\n",
    "        # 3. Forward Pass & Metric Calculation\n",
    "        self.model.eval()\n",
    "        self.model.zero_grad()\n",
    "\n",
    "        # ãƒ•ãƒƒã‚¯å†…ã§ãƒ‡ãƒ¼ã‚¿ã‚’ã‚­ãƒ£ãƒ—ãƒãƒ£ã™ã‚‹ãŸã‚ã®ã‚³ãƒ³ãƒ†ãƒŠ\n",
    "        feature_acts_storage = {}\n",
    "\n",
    "        def atp_hook(activation, hook):\n",
    "            \"\"\"\n",
    "            Activationã‚’å–å¾—ã—ã€SAEã‚’é€šã—ã¦å‹¾é…ã‚’æµã™ãƒ•ãƒƒã‚¯\n",
    "            \"\"\"\n",
    "            # activation: [batch, seq, d_model]\n",
    "            # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆä½ç½®ï¼ˆå›ç­”ç›´å‰ï¼‰ã®ã¿ã‚’æŠ½å‡º\n",
    "            # batch=1 å‰æ\n",
    "            target_act = activation[:, target_pos:target_pos+1, :]\n",
    "\n",
    "            # SAE Encode (Feature Activationè¨ˆç®—)\n",
    "            # SAEã®å…¥åŠ›æ¬¡å…ƒã«åˆã‚ã›ã¦èª¿æ•´\n",
    "            f_acts = self.sae.encode(target_act) # [1, 1, n_features]\n",
    "\n",
    "            # å‹¾é…è¨ˆç®—ã®ãŸã‚ã«ä¿å­˜ (retain_gradé‡è¦)\n",
    "            f_acts.requires_grad_(True)\n",
    "            f_acts.retain_grad()\n",
    "            feature_acts_storage['acts'] = f_acts\n",
    "\n",
    "            # SAE Decode (Reconstruction)\n",
    "            x_hat = self.sae.decode(f_acts)\n",
    "\n",
    "            # Gradient Trick:\n",
    "            # Forward: å…ƒã®Activation (x) ã‚’ãã®ã¾ã¾æµã™ (Teacher Forcingã®ç²¾åº¦ç¶­æŒ)\n",
    "            # Backward: Reconstruction (x_hat) ã‚’é€šã—ã¦å‹¾é…ã‚’æµã™ (SAEç‰¹å¾´é‡ã¸ã®Pathã‚’ä½œã‚‹)\n",
    "            # x_out = x_hat + (x - x_hat).detach()\n",
    "            # ã“ã‚Œã«ã‚ˆã‚Šã€Metricã®å‹¾é…ã¯ x_hat -> f_acts ã¨ä¼æ’­ã™ã‚‹\n",
    "\n",
    "            x_out = x_hat + (target_act - x_hat).detach()\n",
    "\n",
    "            # å…ƒã®ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã«æˆ»ã™\n",
    "            activation[:, target_pos:target_pos+1, :] = x_out\n",
    "            return activation\n",
    "\n",
    "        # ãƒ¢ãƒ‡ãƒ«å®Ÿè¡Œ\n",
    "        try:\n",
    "            logits = self.model.run_with_hooks(\n",
    "                input_tokens,\n",
    "                fwd_hooks=[(self.hook_name, atp_hook)]\n",
    "            )\n",
    "\n",
    "            # 4. Metric Calculation (Logit Difference)\n",
    "            # logit_pos ã®ä½ç½®ã§ã®äºˆæ¸¬ã‚’è¦‹ã‚‹ï¼ˆoffsetã«å¿œã˜ãŸä½ç½®ï¼‰\n",
    "            target_logit = logits[0, logit_pos, target_token_id]\n",
    "            base_logit = logits[0, logit_pos, base_token_id]\n",
    "            metric = target_logit - base_logit\n",
    "\n",
    "            # 5. Backward Pass\n",
    "            metric.backward()\n",
    "\n",
    "            # 6. AtP Score Calculation\n",
    "            # Score = Activation * Gradient\n",
    "            f_acts = feature_acts_storage['acts']\n",
    "            f_grad = f_acts.grad\n",
    "\n",
    "            if f_acts is None or f_grad is None:\n",
    "                return {\"error\": \"Failed to capture gradients\"}\n",
    "\n",
    "            atp_scores = (f_acts * f_grad).detach().cpu().squeeze() # [n_features]\n",
    "\n",
    "            # çµæœã®æŠ½å‡ºï¼ˆTop-K & Non-zeroï¼‰\n",
    "            # ãƒ¡ãƒ¢ãƒªç¯€ç´„ã®ãŸã‚ã€ã‚¹ã‚³ã‚¢ãŒé«˜ã„ã‚‚ã®ã ã‘ã‚’ä¿å­˜\n",
    "            top_k = 50\n",
    "            top_indices = torch.topk(atp_scores.abs(), k=top_k).indices\n",
    "\n",
    "            top_features = []\n",
    "            for idx in top_indices:\n",
    "                idx_val = idx.item()\n",
    "                score = atp_scores[idx_val].item()\n",
    "                activation_val = f_acts[0, 0, idx_val].item()\n",
    "\n",
    "                top_features.append({\n",
    "                    \"id\": str(idx_val),\n",
    "                    \"score\": score,\n",
    "                    \"activation\": activation_val,\n",
    "                    \"gradient\": f_grad[0, 0, idx_val].item()\n",
    "                })\n",
    "\n",
    "            return {\n",
    "                \"status\": \"success\",\n",
    "                \"target_token\": self.model.to_string(target_token_id),\n",
    "                \"base_token\": self.model.to_string(base_token_id),\n",
    "                \"token_position\": token_offset,  # ã©ã®ä½ç½®ã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ä½¿ã£ãŸã‹è¨˜éŒ²\n",
    "                \"logit_diff\": metric.item(),\n",
    "                \"top_features\": top_features\n",
    "            }\n",
    "\n",
    "        except RuntimeError as e:\n",
    "            if \"out of memory\" in str(e).lower():\n",
    "                torch.cuda.empty_cache()\n",
    "                return {\"error\": \"OOM\"}\n",
    "            raise e\n",
    "        finally:\n",
    "            # ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—\n",
    "            self.model.zero_grad()\n",
    "            del feature_acts_storage\n",
    "            if 'logits' in locals(): del logits\n",
    "            if 'f_acts' in locals(): del f_acts\n",
    "            if 'f_grad' in locals(): del f_grad\n",
    "            torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tEYEbjxRqP-R"
   },
   "source": [
    "# ==========================================\n",
    "# 4. ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œã‚¹ã‚¯ãƒªãƒ—ãƒˆ\n",
    "# =========================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1765896935240,
     "user": {
      "displayName": "Kuwahara Itsuki",
      "userId": "08552036371601467574"
     },
     "user_tz": -540
    },
    "id": "--Ye7DiWqSZH"
   },
   "outputs": [],
   "source": [
    "def run_attribution_patching_pipeline(target_layer: int = 31, input_json_path: str = None):\n",
    "    \"\"\"\n",
    "    Attribution Patchingåˆ†æã®ãƒ¡ã‚¤ãƒ³ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³\n",
    "    \n",
    "    Args:\n",
    "        target_layer: åˆ†æå¯¾è±¡ã®å±¤ (9, 16, 20, 31)\n",
    "        input_json_path: å…¥åŠ›JSONãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ï¼ˆæŒ‡å®šã—ãªã„å ´åˆã¯è‡ªå‹•æ¢ç´¢ï¼‰\n",
    "    \"\"\"\n",
    "    # ãƒ‘ã‚¹è¨­å®š\n",
    "    if input_json_path is None:\n",
    "        # labeled_dataãƒ•ã‚©ãƒ«ãƒ€ã‹ã‚‰æœ€æ–°ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¢ã™\n",
    "        search_dir = Path(os.path.join(PROJECT_PATH, \"results/labeled_data\"))\n",
    "        if search_dir.exists():\n",
    "            json_files = list(search_dir.glob(\"*.json\"))\n",
    "            if json_files:\n",
    "                input_json_path = str(sorted(json_files)[-1])\n",
    "                print(f\"ğŸ“‚ è‡ªå‹•æ¤œå‡º: {input_json_path}\")\n",
    "        \n",
    "        if input_json_path is None:\n",
    "            raise FileNotFoundError(\"å…¥åŠ›JSONãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚input_json_pathã‚’æŒ‡å®šã—ã¦ãã ã•ã„ã€‚\")\n",
    "    \n",
    "    INPUT_JSON_PATH = input_json_path\n",
    "\n",
    "    # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ä»˜ãã®å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«åã‚’ç”Ÿæˆï¼ˆå±¤æƒ…å ±ã‚’å«ã‚€ï¼‰\n",
    "    from datetime import datetime\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    OUTPUT_JSON_PATH = os.path.join(\n",
    "        PROJECT_PATH,\n",
    "        f\"results/feedback/atp_results_gemma-2-9b-it_layer{target_layer}_{timestamp}.json\"\n",
    "    )\n",
    "    \n",
    "    print(f\"ğŸ¯ Target Layer: {target_layer}\")\n",
    "\n",
    "    # ãƒ•ã‚¡ã‚¤ãƒ«å­˜åœ¨ç¢ºèª\n",
    "    # å±¤ã«å¿œã˜ãŸConfigã‚’é¸æŠ\n",
    "    from config import (\n",
    "        FEEDBACK_GEMMA2_9B_IT_LAYER9_CONFIG,\n",
    "        FEEDBACK_GEMMA2_9B_IT_LAYER20_CONFIG,\n",
    "        FEEDBACK_GEMMA2_9B_IT_CONFIG  # Layer 31\n",
    "    )\n",
    "    \n",
    "    config_map = {\n",
    "        9: FEEDBACK_GEMMA2_9B_IT_LAYER9_CONFIG,\n",
    "        16: None,  # 16å±¤ã®è¨­å®šãŒå¿…è¦ãªå ´åˆã¯ config.py ã«è¿½åŠ ã—ã¦ãã ã•ã„\n",
    "        20: FEEDBACK_GEMMA2_9B_IT_LAYER20_CONFIG,\n",
    "        31: FEEDBACK_GEMMA2_9B_IT_CONFIG\n",
    "    }\n",
    "    \n",
    "    selected_config = config_map.get(target_layer)\n",
    "    if selected_config is None:\n",
    "        # 16å±¤ãªã©æœªå®šç¾©ã®å ´åˆã¯æ‰‹å‹•ã§è¨­å®šã‚’ä½œæˆ\n",
    "        if target_layer == 16:\n",
    "            print(f\"âš ï¸ Layer {target_layer}ã®è¨­å®šãŒãªã„ãŸã‚ã€å‹•çš„ã«ç”Ÿæˆã—ã¾ã™\")\n",
    "            from config import ExperimentConfig, ModelConfig, DataConfig, GenerationConfig\n",
    "            from config import PromptConfig, AnalysisConfig, VisualizationConfig, DebugConfig, FeedbackConfig\n",
    "            selected_config = ExperimentConfig(\n",
    "                model=ModelConfig(\n",
    "                    name=\"gemma-2-9b-it\",\n",
    "                    sae_release=\"gemma-scope-9b-it-res-canonical\",\n",
    "                    sae_id=f\"layer_{target_layer}/width_16k/canonical\",\n",
    "                    hook_name=f\"blocks.{target_layer}.hook_resid_post\",\n",
    "                    device=\"cuda\",\n",
    "                    use_accelerate=True,\n",
    "                    use_fp16=False,\n",
    "                    use_bfloat16=True,\n",
    "                    low_cpu_mem_usage=True,\n",
    "                    device_map=\"auto\"\n",
    "                )\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Layer {target_layer}ã¯ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚9, 16, 20, 31ã®ã„ãšã‚Œã‹ã‚’æŒ‡å®šã—ã¦ãã ã•ã„ã€‚\")\n",
    "\n",
    "    print(f\"ğŸ”„ Loading Model & SAE for Layer {target_layer}...\")\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    model = HookedTransformer.from_pretrained_no_processing(\n",
    "        selected_config.model.name,\n",
    "        device=device,\n",
    "        dtype=torch.bfloat16\n",
    "    )\n",
    "\n",
    "    # SAEãƒ­ãƒ¼ãƒ‰\n",
    "    sae = SAE.from_pretrained(\n",
    "        release=selected_config.model.sae_release,\n",
    "        sae_id=selected_config.model.sae_id,\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    analyzer = AttributionPatchingAnalyzer(model, sae, selected_config)\n",
    "\n",
    "    # 1. ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿\n",
    "    print(f\"ğŸ“‚ Loading data from {INPUT_JSON_PATH}...\")\n",
    "    with open(INPUT_JSON_PATH, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # 2. ãƒ¡ã‚¤ãƒ³ãƒ«ãƒ¼ãƒ—\n",
    "    samples = list(yield_sycophancy_samples(data))\n",
    "    print(f\"ğŸš€ Starting ATP analysis for {len(samples)} samples...\")\n",
    "    print(f\"ğŸ’¾ Results will be saved to: {OUTPUT_JSON_PATH}\")\n",
    "\n",
    "    for i, sample in enumerate(tqdm(samples)):\n",
    "        res = analyzer.calculate_atp_for_sample(sample)\n",
    "\n",
    "        # å…ƒã®JSONã«çµæœã‚’çµ±åˆ\n",
    "        question_id = sample[\"question_id\"]\n",
    "        variation_idx = sample[\"variation_index\"]\n",
    "\n",
    "        # ãƒ‡ãƒãƒƒã‚°: çµæœã®å†…å®¹ã‚’è¡¨ç¤ºï¼ˆæœ€åˆã®æ•°ã‚µãƒ³ãƒ—ãƒ«ã®ã¿ï¼‰\n",
    "        if i < 3:\n",
    "            print(f\"\\nğŸ” Sample {i} result: {res}\")\n",
    "\n",
    "        # è©²å½“ã™ã‚‹variationã‚’æ¢ã—ã¦ atp_analysis ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’è¿½åŠ \n",
    "        for result in data[\"results\"]:\n",
    "            if result[\"question_id\"] == question_id:\n",
    "                variations = result[\"variations\"]\n",
    "                if variation_idx < len(variations):\n",
    "                    if res.get(\"status\") == \"success\":\n",
    "                        variations[variation_idx][\"atp_analysis\"] = {\n",
    "                            \"top_features\": res[\"top_features\"],\n",
    "                            \"target_token\": res[\"target_token\"],\n",
    "                            \"base_token\": res[\"base_token\"],\n",
    "                            \"token_position\": res[\"token_position\"],\n",
    "                            \"logit_diff\": res[\"logit_diff\"]\n",
    "                        }\n",
    "                    else:\n",
    "                        # ã‚¨ãƒ©ãƒ¼ã®å ´åˆã€è©³ç´°æƒ…å ±ã‚‚ä¿å­˜\n",
    "                        variations[variation_idx][\"atp_analysis\"] = {\n",
    "                            \"error\": res.get(\"error\") or res.get(\"skipped\") or \"unknown\",\n",
    "                            \"details\": res\n",
    "                        }\n",
    "                        # æœ€åˆã®ã‚¨ãƒ©ãƒ¼ã‚’è¡¨ç¤º\n",
    "                        if i < 10:\n",
    "                            print(f\"âš ï¸ Sample {i} (Q{question_id}, Var{variation_idx}): {res}\")\n",
    "                break\n",
    "\n",
    "        # å®šæœŸçš„ã«ä¿å­˜\n",
    "        if (i + 1) % 10 == 0:\n",
    "            with open(OUTPUT_JSON_PATH, 'w', encoding='utf-8') as f:\n",
    "                json.dump(data, f, indent=2, ensure_ascii=False)\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    # æœ€çµ‚ä¿å­˜\n",
    "    with open(OUTPUT_JSON_PATH, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    print(f\"âœ… Analysis completed. Saved to {OUTPUT_JSON_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3207,
     "status": "ok",
     "timestamp": 1765896940034,
     "user": {
      "displayName": "Kuwahara Itsuki",
      "userId": "08552036371601467574"
     },
     "user_tz": -540
    },
    "id": "MXu0pcMiqclK",
    "outputId": "704a3cd7-3150-4142-f716-1f8f21ec22d8"
   },
   "outputs": [],
   "source": [
    "# å®Ÿè¡Œ\n",
    "if __name__ == \"__main__\":\n",
    "    # Colabç’°å¢ƒã§ã®å®Ÿè¡Œã‚’æƒ³å®š\n",
    "    # \n",
    "    # ä½¿ç”¨ä¾‹:\n",
    "    # run_attribution_patching_pipeline(target_layer=31)  # 31å±¤ã§å®Ÿè¡Œ\n",
    "    # run_attribution_patching_pipeline(target_layer=20, input_json_path=\"/path/to/file.json\")  # 20å±¤ã€ãƒ•ã‚¡ã‚¤ãƒ«æŒ‡å®š\n",
    "    \n",
    "    # ğŸ“Œ ã“ã“ã§å±¤ã‚’é¸æŠã—ã¦ãã ã•ã„ (9, 16, 20, 31)\n",
    "    TARGET_LAYER = 31\n",
    "    \n",
    "    # ğŸ“Œ å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æŒ‡å®šã™ã‚‹å ´åˆã¯ã“ã“ã‚’å¤‰æ›´ï¼ˆNoneã®å ´åˆã¯è‡ªå‹•æ¤œå‡ºï¼‰\n",
    "    INPUT_FILE = None  # ä¾‹: \"/content/drive/MyDrive/sae_pj2/results/labeled_data/xxx.json\"\n",
    "    \n",
    "    try:\n",
    "        run_attribution_patching_pipeline(\n",
    "            target_layer=TARGET_LAYER,\n",
    "            input_json_path=INPUT_FILE\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error during execution: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kAPtlItTqYiT"
   },
   "source": [
    "# ==========================================\n",
    "# 4. å®Ÿè¡Œ\n",
    "# ==========================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hzOlPlU81SaR"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0f63b4e1786948109185728b22d2e25e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "32e298d4dc404fd4a6518da56b3ec839": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3b8a438247c249c69952d326f51eb97a",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_f9152bb3863840ef9a00fe2aaeaebf40",
      "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
     }
    },
    "3b8a438247c249c69952d326f51eb97a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3dc871af7cef4277913a24cca68bbcf3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "PasswordModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "PasswordModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "PasswordView",
      "continuous_update": true,
      "description": "Token:",
      "description_tooltip": null,
      "disabled": false,
      "layout": "IPY_MODEL_3eae439ff8954cdc9d305c8ec0fbc8d4",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_0f63b4e1786948109185728b22d2e25e",
      "value": ""
     }
    },
    "3eae439ff8954cdc9d305c8ec0fbc8d4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "40e5bc5bc2d54363be268930c8671d50": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "443fdb2fc93342dfbabbb55c7f1e0f01": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": "center",
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": "flex",
      "flex": null,
      "flex_flow": "column",
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "50%"
     }
    },
    "60c27a47951a49ef9c02ee6678507744": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6180f6670490493c89186793d038c996": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9037406cb747438f8b7a3baad0649eb2",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_60c27a47951a49ef9c02ee6678507744",
      "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
     }
    },
    "6aa3e365a24b4ce29b07ef7dcd39611e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "74162eea75ac4138bbb8b7144a806f4d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ButtonStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ButtonStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "button_color": null,
      "font_weight": ""
     }
    },
    "8a9ffec5f8484b4195edcfdbe964046e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "VBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "VBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "VBoxView",
      "box_style": "",
      "children": [],
      "layout": "IPY_MODEL_443fdb2fc93342dfbabbb55c7f1e0f01"
     }
    },
    "9037406cb747438f8b7a3baad0649eb2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "927b578a4aa04571b4bcf70556facac0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b0dd67f7c11d4626a9b960a437904168": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bce6a0cd6bc54bfab26c399142d4b5ea": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "LabelModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "LabelModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "LabelView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_927b578a4aa04571b4bcf70556facac0",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_e8d99475cdbd4967b8ee478d74cfc772",
      "value": "Connecting..."
     }
    },
    "c387fc8f99c841d08e8161848ae06c74": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "CheckboxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "CheckboxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "CheckboxView",
      "description": "Add token as git credential?",
      "description_tooltip": null,
      "disabled": false,
      "indent": true,
      "layout": "IPY_MODEL_40e5bc5bc2d54363be268930c8671d50",
      "style": "IPY_MODEL_6aa3e365a24b4ce29b07ef7dcd39611e",
      "value": true
     }
    },
    "e8d99475cdbd4967b8ee478d74cfc772": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f3e319f521e841e0983d9c0664424def": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ButtonModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ButtonModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ButtonView",
      "button_style": "",
      "description": "Login",
      "disabled": false,
      "icon": "",
      "layout": "IPY_MODEL_b0dd67f7c11d4626a9b960a437904168",
      "style": "IPY_MODEL_74162eea75ac4138bbb8b7144a806f4d",
      "tooltip": ""
     }
    },
    "f9152bb3863840ef9a00fe2aaeaebf40": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
